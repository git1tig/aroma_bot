# -*- coding: utf-8 -*-
import os
import telebot
import openai
from dotenv import load_dotenv
import pandas as pd
import requests
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain.schema import Document  # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç

# === –ó–ê–ì–†–£–ó–ö–ê API-–ö–õ–Æ–ß–ï–ô ===
load_dotenv()
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not TELEGRAM_BOT_TOKEN or not OPENAI_API_KEY:
    raise ValueError("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç TELEGRAM_BOT_TOKEN –∏–ª–∏ OPENAI_API_KEY –≤ .env —Ñ–∞–π–ª–µ!")

openai.api_key = OPENAI_API_KEY
bot = telebot.TeleBot(TELEGRAM_BOT_TOKEN)

# === –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú ===
MASLA_FILE = "/app/mono_oils.txt"
FAISS_INDEX_FILE = "/app/index.faiss"

# === –ü–†–û–í–ï–†–ö–ê –ò –ó–ê–ì–†–£–ó–ö–ê FAISS ===
embs = OpenAIEmbeddings()
db = None  # –û–±—ä—è–≤–ª—è–µ–º db –∑–∞—Ä–∞–Ω–µ–µ

if os.path.exists(FAISS_INDEX_FILE):
    print("‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–∑ —Ñ–∞–π–ª–∞...")
    db = FAISS.load_local(FAISS_INDEX_FILE, embs, allow_dangerous_deserialization=True)
else:
    print("‚ö†Ô∏è –§–∞–π–ª FAISS-—Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ...")

    if not os.path.exists(MASLA_FILE):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª {MASLA_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω! –î–æ–±–∞–≤—å –µ–≥–æ –≤ –∫–∞—Ç–∞–ª–æ–≥.")

    with open(MASLA_FILE, 'r', encoding='utf-8') as f:
        my_text = f.read()

    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "Header 1")])
    chunks = splitter.split_text(my_text)  # ‚úÖ `split_text()` —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω—É–∂–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã


    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å OpenAI Embeddings
    db = FAISS.from_documents(chunks, embs)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
    db.save_local(FAISS_INDEX_FILE)
    print("‚úÖ FAISS-—Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ–∑–¥–∞–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")

# === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó GOOGLE SHEETS (UTF-8) ===
SHEET_URL = "https://docs.google.com/spreadsheets/d/1MknmvI9_YvjM7bge9tPryRKrrzCi-Ywc5ZfYiyb6Bdg/export?format=csv"
COLUMN_NAMES = ["Name", "Vol", "Price"]

try:
    df = pd.read_csv(SHEET_URL, names=COLUMN_NAMES, encoding='utf-8')
    print("‚úÖ –î–∞–Ω–Ω—ã–µ –æ –º–∞—Å–ª–∞—Ö —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
except Exception as e:
    print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö:", e)
    df = pd.DataFrame(columns=COLUMN_NAMES)  # –ü—É—Å—Ç–∞—è —Ç–∞–±–ª–∏—Ü–∞, –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å

# === GPT-–§–£–ù–ö–¶–ò–ò ===
s1 = "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –º–∞—Å–µ–ª..."
s2 = "–û–ø—Ä–µ–¥–µ–ª–∏, –∫–∞–∫–æ–µ –º–∞—Å–ª–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –ø–æ–¥ –∑–∞–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞..."

def gpt_for_query(prompt, system):
    response = openai.chat.completions.create(
        model='gpt-4o-mini',
        messages=[{"role": "system", "content": system},
                  {"role": "user", "content": prompt}],
        temperature=1
    )
    return response.choices[0].message.content

# === –¢–ï–õ–ï–ì–†–ê–ú-–ë–û–¢ ===
user_states = {}

WAITING_OIL_NAME = "waiting_for_oil"
WAITING_NEXT_OIL = "waiting_for_next_oil"

@bot.message_handler(commands=['—Ä'])
def oil_command(message):
    bot.reply_to(message, "–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–∞—Å–ª–∞ ('*' - –∑–∞–∫–æ–Ω—á–∏—Ç—å –≤–≤–æ–¥):")
    user_states[message.chat.id] = WAITING_NEXT_OIL

@bot.message_handler(commands=['–º'])
def oil_command(message):
    bot.reply_to(message, "–í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–∞—Å–ª–∞:")
    user_states[message.chat.id] = WAITING_OIL_NAME

@bot.message_handler(commands=['—Å—Ç–æ–ø'])
def cancel_command(message):
    bot.reply_to(message, "–ö–æ–º–∞–Ω–¥–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞. –ù–∞—á–Ω–∏—Ç–µ –∑–∞–Ω–æ–≤–æ —Å /–º.")
    user_states.pop(message.chat.id, None)

# === –û–¢–ü–†–ê–í–ö–ê –î–õ–ò–ù–ù–´–• –°–û–û–ë–©–ï–ù–ò–ô ===
MAX_MESSAGE_LENGTH = 4000  # –ß—É—Ç—å –º–µ–Ω—å—à–µ 4096, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫

def send_long_message(chat_id, text):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —á–∞—Å—Ç—è–º–∏."""
    for i in range(0, len(text), MAX_MESSAGE_LENGTH):
        bot.send_message(chat_id, text[i:i + MAX_MESSAGE_LENGTH])

@bot.message_handler(func=lambda message: True)
def handle_input(message):
    print(f"üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {message.text}")  # <--- –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    user_input = message.text.strip().lower()

    if message.chat.id in user_states:
        if user_states[message.chat.id] == WAITING_OIL_NAME:
            if db:
                docs = db.similarity_search_with_score(user_input, k=1)
                if docs[0][1] < 0.37:
                    bot.reply_to(message, f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ {user_input}: {docs[0][0].page_content}")
                else:
                    docs = db.similarity_search(gpt_for_query(user_input, s2), k=1)
                    bot.reply_to(message, f'–ü–æ–¥ –≤–∞—à –∑–∞–ø—Ä–æ—Å {user_input} –ø–æ–¥—Ö–æ–¥–∏—Ç —ç—Ç–æ: {docs[0].page_content}')
            else:
                bot.reply_to(message, "‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö FAISS –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
    else:
        if db:
            gpt_generated = gpt_for_query(message.text, s1)
            docs = db.similarity_search(gpt_generated, k=5)
            response_text = "\n".join([doc.page_content for doc in docs])

            if len(response_text) > MAX_MESSAGE_LENGTH:
                send_long_message(message.chat.id, response_text)
            else:
                bot.reply_to(message, response_text)
        else:
            bot.reply_to(message, "‚ö†Ô∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö FAISS –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")

if __name__ == "__main__":
    print("ü§ñ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω! –û–∂–∏–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è...")
    bot.infinity_polling()
